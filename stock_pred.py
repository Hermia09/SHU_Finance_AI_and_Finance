# -*- coding: utf-8 -*-
"""StockPred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WY2CU2BO5YTsli-CFyWL_g4qjufMLmbt
"""

import os
os.getcwd()

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold,StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
import random
random.seed(1)
data0=pd.read_csv('2330.csv')
data0_train=data0.iloc[0:291,:]
data0_test=data0.iloc[292:309,:]
train_X=data0_train[['Volume','MA5','MA10','OSC','D(9,3)','K(9,3)','RSI_6']]
train_Y=data0_train['LABAL']
test_X=data0_test[['Volume','MA5','MA10','OSC','D(9,3)','K(9,3)','RSI_6']]
test_Y=data0_test['LABAL']

#決策樹
tree= DecisionTreeClassifier(criterion='gini',max_depth=5) #以gini部純度切割，max_depth=5只最多到五層
tree.fit(train_X, train_Y)
pred_Y_tree = tree.predict(test_X)
pred_acc_tree = accuracy_score(test_Y,pred_Y_tree) #test_Y是真實得資料
loss_Y_tree = tree.predict(train_X) #看樣本外的能力
loss_acc_tree = accuracy_score(train_Y,loss_Y_tree)
print(pred_acc_tree, loss_acc_tree) #看樣本內的能力

#貝氏分類器
bayes = GaussianNB()
bayes.fit(train_X, train_Y)
pred_Y_bayes = bayes.predict(test_X)
pred_acc_bayes = accuracy_score(test_Y,pred_Y_bayes)
loss_Y_bayes = bayes.predict(train_X)
loss_acc_bayes = accuracy_score(train_Y,loss_Y_bayes)
print(pred_acc_bayes, loss_acc_bayes)

#SVM(linear)
param_list1 = {'C': [0.1, 0.25, 0.5 ,0.75, 1, 3, 5, 7, 9, 15, 20, 100, 500]}
cv_svm1 = GridSearchCV(SVC(kernel='linear'), param_list1)
cv_svm1.fit(train_X, train_Y)
best_C_svm1=cv_svm1.best_estimator_.C
best_C_svm1

svm1 = SVC(kernel='linear',C=best_C_svm1)
svm1.fit(train_X,train_Y)
pred_Y_svm1 = svm1.predict(test_X)
pred_acc_svm1 = accuracy_score(test_Y,pred_Y_svm1)
loss_Y_svm1 = svm1.predict(train_X)
loss_acc_svm1 = accuracy_score(train_Y,loss_Y_svm1)
print(pred_acc_svm1, loss_acc_svm1)

#SVM(nonlinear)
param_list2 = {'C': [0.1, 0.5, 1, 5, 10, 15, 20, 50, 100],'gamma': [0.01, 0.1 ,0.25, 0.5,0.75, 1, 3, 5, 10, 20]}
cv_svm2 = GridSearchCV(SVC(kernel='rbf'), param_list2)
cv_svm2.fit(train_X, train_Y)
best_C_svm2=cv_svm2.best_estimator_.C
best_gamma_svm2=cv_svm2.best_estimator_.gamma
svm2 = SVC(kernel='rbf',C=best_C_svm2, gamma=best_gamma_svm2)
svm2.fit(train_X,train_Y)
pred_Y_svm2 = svm2.predict(test_X)
pred_acc_svm2 = accuracy_score(test_Y,pred_Y_svm2)
loss_Y_svm2 = svm2.predict(train_X)
loss_acc_svm2 = accuracy_score(train_Y,loss_Y_svm2)
print(pred_acc_svm2, loss_acc_svm2)

#Logistic Regression
logist = LogisticRegression()
logist.fit(train_X, train_Y)
pred_Y_logist = logist.predict(test_X)
pred_acc_logist = accuracy_score(test_Y,pred_Y_logist)
loss_Y_logist = logist.predict(train_X)
loss_acc_logist = accuracy_score(train_Y,loss_Y_logist)
print(pred_acc_logist, loss_acc_logist)

#隨機森林
forest = RandomForestClassifier(n_estimators=500) #可以挑幾棵樹 100~500
forest.fit(train_X,train_Y)
pred_Y_forest = forest.predict(test_X)
pred_acc_forest = accuracy_score(test_Y,pred_Y_forest)
loss_Y_forest = logist.predict(train_X)
loss_acc_forest = accuracy_score(train_Y,loss_Y_forest)
print(pred_acc_forest, loss_acc_forest)

# KNN
knn = KNeighborsClassifier(n_neighbors=3) #可以挑幾棵樹 100~500
knn.fit(train_X,train_Y)
pred_Y_knn = knn.predict(test_X)
pred_acc_knn = accuracy_score(test_Y,pred_Y_knn)
loss_Y_knn = knn.predict(train_X)
loss_acc_knn = accuracy_score(train_Y,loss_Y_knn)
print(pred_acc_knn, loss_acc_knn)

pred_res=[pred_acc_tree,pred_acc_bayes,pred_acc_svm1,pred_acc_svm2,pred_acc_logist,pred_acc_forest,pred_acc_knn]
loss_res=[loss_acc_tree,loss_acc_bayes,loss_acc_svm1,loss_acc_svm2,loss_acc_logist,loss_acc_forest,loss_acc_knn]
print(pred_res)
print(loss_res)

